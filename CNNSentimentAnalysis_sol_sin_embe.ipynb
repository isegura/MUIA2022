{"cells":[{"cell_type":"markdown","metadata":{"id":"4btRBQljoPw8"},"source":["<a href=\"http://colab.research.google.com/github/dipanjanS/nlp_workshop_odsc19/blob/master/Module05%20-%20NLP%20Applications/Project07B%20-%20Text%20Classification%20Deep%20Learning%20CNN%20Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"f64bBXyhpxTR"},"source":["# CNN applied to sentiment analysis\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CmAi-Ykk_2Ph"},"source":["## Exercise\n","\n","- 1) What does it happen if you add more convolutional layers to your model? Do the results improve?\n","- 2) What does it happen if the network is inistialized with pre-trained word embeddings instead of using random initialization? Do the results improve?\n","- 3) Plot the learning curves. \n"," "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hrJ4DwicdSF6","outputId":"a40da8d2-3a62-4663-feb9-5821a2a3ec26","executionInfo":{"status":"ok","timestamp":1654541853649,"user_tz":-120,"elapsed":4312,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/movie_reviews.zip.\n","instances: 2000 2000\n"]}],"source":["import nltk\n","nltk.download('movie_reviews')\n","from nltk.corpus import movie_reviews\n","\n","\n","X=[] #list to save the texts\n","y=[] #list to save the labels\n","\n","\n","for file_id in movie_reviews.fileids(): #this traverse all movides\n","    category=movie_reviews.categories(file_id)\n","    label=category[0]\n","    tokens=list(movie_reviews.words(file_id))\n","    text=' '.join(str(word) for word in tokens)\n","    X.append(text)\n","    y.append(label)\n","\n","X=list(X)\n","print(\"instances:\",len(X),len(y))\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"JNtOhrOtfKYs","executionInfo":{"status":"ok","timestamp":1654541853650,"user_tz":-120,"elapsed":13,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n"]},{"cell_type":"markdown","metadata":{"id":"IAa0ulL6rfUI"},"source":["We could translate each class (\"pos\", and \"neg\") to 1 or 0 in the previous for.\n","However, sklearn already provides us a class to translate string classes to numbers or vectors. "]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ifh29eFWqEYw","outputId":"a33ba1ef-963f-480b-be6a-07ad7710e007","executionInfo":{"status":"ok","timestamp":1654541853651,"user_tz":-120,"elapsed":12,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of positive and negative reviews in the training: 757 743\n","Number of positive and negative reviews in the test: 243 257\n"]}],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","num_classes=2 # positive -> 1, negative -> 0\n","y_train = le.fit_transform(y_train)\n","y_test = le.transform(y_test)\n","\n","print('Number of positive and negative reviews in the training:', sum(y_train), len(y_train)-sum(y_train))\n","print('Number of positive and negative reviews in the test:', sum(y_test), len(y_test)-sum(y_test))\n"]},{"cell_type":"markdown","metadata":{"id":"xd60z6QKg_Lp"},"source":["The baseline system based on tf-idf model and the classifier SVM achieved an accuracy of 0.82, F1(1) 0.82."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dff8sG63cw03","outputId":"2d4d432e-2217-4604-fdc7-6e080a7c44e4","executionInfo":{"status":"ok","timestamp":1654541857134,"user_tz":-120,"elapsed":3491,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size of the training=35192\n","Number of Documents in the training=1500\n"]},{"output_type":"execute_result","data":{"text/plain":["((1500, 1000), (500, 1000))"]},"metadata":{},"execution_count":4}],"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","\n","tokenizer = Tokenizer(oov_token='<UNK>')\n","# fit the tokenizer on the documents\n","tokenizer.fit_on_texts(X_train)\n","\n","#we add a word index for the pad tokens\n","tokenizer.word_index['<PAD>'] = 0\n","print(\"Vocabulary size of the training={}\".format(len(tokenizer.word_index)))\n","print(\"Number of Documents in the training={}\".format(tokenizer.document_count))\n","\n","#We now transform the words to indexes\n","train_sequences = tokenizer.texts_to_sequences(X_train)\n","test_sequences = tokenizer.texts_to_sequences(X_test)\n","#print(X_train[0].split())\n","#print(train_sequences[0])\n","\n","MAX_SEQUENCE_LENGTH = 1000 #most texts have less than 1000 tokens\n","\n","# pad dataset to a maximum review length in words\n","\n","train_seq_pad = sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","test_seq_pad = sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","train_seq_pad.shape, test_seq_pad.shape"]},{"cell_type":"markdown","metadata":{"id":"YCjRYBh2pxUM"},"source":["## Prepare the Model\n","\n","Since textual data is a sequence of words, we utilize ```1D``` convolutions to scan through the sentences.\n","The model first transforms each word into lower dimensional embedding/vector space followed by 1d convolutions and then passing the data through dense layers before the final layer for classification"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"DQH9OC9EX9EZ"}},{"cell_type":"code","source":["import tensorflow\n","print(tensorflow.__version__)\n"],"metadata":{"id":"4seNvNM_X93j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654541857134,"user_tz":-120,"elapsed":12,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}},"outputId":"7749bb53-b2c3-44a8-b650-f8b11ba976b4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.2\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"1B2VZAc9vAIP","executionInfo":{"status":"ok","timestamp":1654541857135,"user_tz":-120,"elapsed":8,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Conv1D\n","from keras.layers import MaxPooling1D\n","from keras.layers import Embedding\n","\n","import numpy as np\n","\n","# fix random seed for reproducibility\n","import tensorflow as tf\n","tf.random.set_seed(1)"]},{"cell_type":"markdown","metadata":{"id":"cCtm4fRewuhO"},"source":["We now create the matrix of word embeddings"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"LR3mdd8kjgW1","executionInfo":{"status":"ok","timestamp":1654541857135,"user_tz":-120,"elapsed":7,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"outputs":[],"source":["VOCAB_SIZE = len(tokenizer.word_index)\n","\n","EPOCHS=20\n","BATCH_SIZE=16\n","#Set True if you want to use pre-trained word embeddings to initialize the inputs\n","#False eoc\n","USEWE=False\n","EMBED_SIZE = 300\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"rf9htfCRxJVQ","executionInfo":{"status":"ok","timestamp":1654541857736,"user_tz":-120,"elapsed":608,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"outputs":[],"source":["if USEWE:\n","    import gensim.downloader as api\n","    modelWE = api.load(\"glove-wiki-gigaword-50\")\n","    EMBED_SIZE = 50\n","\n","    #modelWE = api.load(\"word2vec-google-news-300\")\n","    #EMBED_SIZE = 300\n","\n","    # create a weight matrix for words in training docs\n","    embedding_matrix = np.zeros((VOCAB_SIZE, EMBED_SIZE))\n","    for word, i in tokenizer.word_index.items():\n","        try:\n","            embedding_vector = modelWE[word]\n","            embedding_matrix[i] = embedding_vector\n","        except:\n","            #if word does not exist, we do not udpate the matrix\n","            pass\n","\n","    print('embedding matrix created')\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXhAERVeXhmZ","outputId":"e1fa9aec-8f5e-461d-9053-9ea93165d91f","executionInfo":{"status":"ok","timestamp":1654541863427,"user_tz":-120,"elapsed":5694,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 1000, 300)         10557600  \n","                                                                 \n"," conv1d (Conv1D)             (None, 1000, 128)         153728    \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 500, 128)         0         \n"," )                                                               \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 500, 128)          49280     \n","                                                                 \n"," max_pooling1d_1 (MaxPooling  (None, 250, 128)         0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 250, 128)          32896     \n","                                                                 \n"," max_pooling1d_2 (MaxPooling  (None, 125, 128)         0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 125, 64)           32832     \n","                                                                 \n"," max_pooling1d_3 (MaxPooling  (None, 62, 64)           0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_4 (Conv1D)           (None, 62, 64)            12352     \n","                                                                 \n"," max_pooling1d_4 (MaxPooling  (None, 31, 64)           0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_5 (Conv1D)           (None, 31, 64)            8256      \n","                                                                 \n"," max_pooling1d_5 (MaxPooling  (None, 15, 64)           0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_6 (Conv1D)           (None, 15, 32)            8224      \n","                                                                 \n"," max_pooling1d_6 (MaxPooling  (None, 7, 32)            0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_7 (Conv1D)           (None, 7, 32)             3104      \n","                                                                 \n"," max_pooling1d_7 (MaxPooling  (None, 3, 32)            0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_8 (Conv1D)           (None, 3, 32)             2080      \n","                                                                 \n"," max_pooling1d_8 (MaxPooling  (None, 1, 32)            0         \n"," 1D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 32)                0         \n","                                                                 \n"," dense (Dense)               (None, 256)               8448      \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 257       \n","                                                                 \n","=================================================================\n","Total params: 10,869,057\n","Trainable params: 10,869,057\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# create the model\n","model = Sequential()\n","if USEWE:\n","    model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=MAX_SEQUENCE_LENGTH,\n","                        weights=[embedding_matrix]), trainable=False)\n","else:\n","    model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=MAX_SEQUENCE_LENGTH))\n","\n","# relu max(0,a), siendo a=WX+b, grandiente es constante\n","# https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/\n","# Two major benefits of ReLUs are velocity and sparsity\n","\n","model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","\n","model.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","\n","model.add(Flatten())\n","model.add(Dense(256, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"szNl8QiQpxUa"},"source":["## Model Training"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0uc0jXszf5ob","outputId":"131c0a3b-471c-455f-d11b-02b831181a0b","executionInfo":{"status":"ok","timestamp":1654541902001,"user_tz":-120,"elapsed":38590,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","85/85 [==============================] - 18s 26ms/step - loss: 0.6938 - accuracy: 0.4748 - val_loss: 0.6931 - val_accuracy: 0.5067\n","Epoch 2/20\n","85/85 [==============================] - 2s 20ms/step - loss: 0.6935 - accuracy: 0.5111 - val_loss: 0.6933 - val_accuracy: 0.4933\n","Epoch 3/20\n","85/85 [==============================] - 2s 20ms/step - loss: 0.6936 - accuracy: 0.5030 - val_loss: 0.6930 - val_accuracy: 0.5067\n","Epoch 4/20\n","85/85 [==============================] - 2s 19ms/step - loss: 0.6941 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.4933\n","Epoch 5/20\n","85/85 [==============================] - 2s 19ms/step - loss: 0.6936 - accuracy: 0.4926 - val_loss: 0.6931 - val_accuracy: 0.5067\n","Epoch 6/20\n","85/85 [==============================] - 2s 20ms/step - loss: 0.6932 - accuracy: 0.4874 - val_loss: 0.6919 - val_accuracy: 0.5067\n","Epoch 7/20\n","85/85 [==============================] - 2s 20ms/step - loss: 0.6804 - accuracy: 0.5341 - val_loss: 0.6942 - val_accuracy: 0.4933\n","Epoch 8/20\n","85/85 [==============================] - 2s 19ms/step - loss: 0.6925 - accuracy: 0.5037 - val_loss: 0.6904 - val_accuracy: 0.5067\n","Epoch 9/20\n","85/85 [==============================] - 2s 19ms/step - loss: 0.5256 - accuracy: 0.7459 - val_loss: 0.3999 - val_accuracy: 0.8333\n","Epoch 10/20\n","85/85 [==============================] - 2s 19ms/step - loss: 0.0990 - accuracy: 0.9704 - val_loss: 0.3596 - val_accuracy: 0.8267\n","Epoch 11/20\n","85/85 [==============================] - 2s 20ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.6943 - val_accuracy: 0.8067\n","Epoch 12/20\n","85/85 [==============================] - 2s 19ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7344 - val_accuracy: 0.8333\n","Epoch 13/20\n","85/85 [==============================] - 2s 20ms/step - loss: 2.9314e-04 - accuracy: 1.0000 - val_loss: 0.7441 - val_accuracy: 0.8333\n"]}],"source":["from keras.callbacks import EarlyStopping\n","earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n","\n","\n","# Fit the model\n","history=model.fit(train_seq_pad, y_train, \n","          validation_split=0.1,\n","          epochs=EPOCHS, \n","          batch_size=BATCH_SIZE, \n","          verbose=1, callbacks=[earlyStopping]\n","          )"]},{"cell_type":"markdown","metadata":{"id":"cuKczZqYpxUk"},"source":["## Model Evaluation"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"3Zik9CWQgNlK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ebf9e966-566b-4867-f1a4-bf8dcd69eb0e","executionInfo":{"status":"ok","timestamp":1654541907320,"user_tz":-120,"elapsed":5337,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["16/16 [==============================] - 1s 19ms/step - loss: 1.1944 - accuracy: 0.7760\n","Accuracy: 77.60%\n"]}],"source":["# Final evaluation of the model\n","scores = model.evaluate(test_seq_pad, y_test, verbose=1)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"B904TLKNiA1B","colab":{"base_uri":"https://localhost:8080/"},"outputId":"790e6a49-7a08-4e75-f0ba-2bcc5c3ce509","executionInfo":{"status":"ok","timestamp":1654541907854,"user_tz":-120,"elapsed":539,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 1, 1, 1, 1, 1, 1, 1, 0, 1]"]},"metadata":{},"execution_count":12}],"source":["predictions=model.predict(test_seq_pad) \n","predictions.reshape(-1)\n","predictions = [1 if item >= 0.5 else 0 for item in predictions]\n","predictions[:10]"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"mINpo7mDpxVC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"44ba6a22-3322-454f-c47e-9760104bc67e","executionInfo":{"status":"ok","timestamp":1654541907855,"user_tz":-120,"elapsed":9,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.82      0.72      0.77       257\n","           1       0.74      0.84      0.78       243\n","\n","    accuracy                           0.78       500\n","   macro avg       0.78      0.78      0.78       500\n","weighted avg       0.78      0.78      0.78       500\n","\n"]}],"source":["from sklearn.metrics import confusion_matrix, classification_report\n","print(classification_report(y_test, predictions))\n"]}],"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"collapsed_sections":[],"name":"CNNSentimentAnalysis_sol_sin_embe.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":0}